<!DOCTYPE html>
<html>
<head>
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1">
<title>Gradient Descent | Machine Learning</title>
<link href="../assets/lib/bootstrap-3.2.0/css/bootstrap.min.css" media="screen" rel="stylesheet" type="text/css">
<link href="../assets/stylesheets/style.css" media="screen" rel="stylesheet" type="text/css">
<link href="../assets/stylesheets/github.css" media="screen" rel="stylesheet" type="text/css">
<script src="../assets/lib/jquery-1.11.1/jquery.min.js"></script>
<script src="../assets/lib/bootstrap-3.2.0/js/bootstrap.min.js"></script>
<script src="../assets/javascripts/jquery.toctree.js"></script>
<script>

(function($) {

    $(function() {
        $('#toc').toctree({ selector:'#content :header', offset:1, depth:3 });
    });

})(window.jQuery);

</script>
</head>
<body>
<div id="wrapper">
    <div id="header">
    <div class="navbar navbar-static-top navbar-inverse">
      <div class="container-fluid">
        <div class="navbar-header">
          <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#jsNavbarCollapse-1">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
          </button>
          <a class="navbar-brand" href="../index.html">Wiki</a>
        </div>
        <div class="collapse navbar-collapse" id="jsNavbarCollapse-1">
          <ul class="nav navbar-nav">
            <li><a href="./index.html">Machine Learning</a></li>
          </ul>
          <form class="navbar-form navbar-right" action="http://www.google.com/search" style="box-shadow: none; border: none">
  <input type="hidden" name="as_sitesearch" value="tachesimazzoca.github.io/wiki">
  <div class="form-group">
    <div class="input-group">
      <div class="input-group-addon" style="background: transparent; border-color: #333">
        <span class="glyphicon glyphicon-search"></span>
      </div>
      <input type="text" class="form-control" name="as_q" placeholder="Search">
    </div>
  </div>
</form>

        </div>
      </div>
    </div>
  </div>
  <!--/#header-->


  <div id="main">
    <div class="container-fluid">
      <ul class="breadcrumb">
        <li><a href="./index.html">Machine Learning</a></li><li>Gradient Descent</li>
      </ul>
    </div>
    <div class="container-fluid">
      <div class="row">
        <div class="col-md-3 col-sm-4 hidden-xs">
          <div id="navigation">
            <div class="panel panel-default">
              <div class="panel-heading">
                <h3 class="panel-title" style="text-align: center; font-size: 11px; text-transform: uppercase; color: #999">Table of Contents</h3>
              </div>
              <div class="panel-body">
              <ul class="nav nav-pills nav-stacked"><li><a href="./overview.html">Overview</a></li><li><a href="./linear_regression.html">Linear Regression</a></li><li><a href="./logistic_regression.html">Logistic Regression</a></li><li class="active"><a href="./gradient_descent.html">Gradient Descent</a></li><li><a href="./regularization.html">Regularization</a></li><li><a href="./neural_networks.html">Neural Networks</a></li><li><a href="./svm.html">Support Vector Machine</a></li><li><a href="./clustering.html">Clustering</a></li><li><a href="./pca.html">Principal Component Analysis</a></li><li><a href="./anomaly_detection.html">Anomaly Detection</a></li><li><a href="./collaborative_filtering.html">Collaborative Filtering</a></li></ul>
              </div>
            </div>
          </div>
        </div>
        <div class="col-md-9 col-sm-8">
          
          <div class="btn-group pull-right">
            <a class="btn btn-default" href="https://raw.github.com/tachesimazzoca/wiki/master/src/machine_learning/gradient_descent.md">Raw</a>
            <a class="btn btn-default" href="https://github.com/tachesimazzoca/wiki/blame/master/src/machine_learning/gradient_descent.md">Blame</a>
            <a class="btn btn-default" href="https://github.com/tachesimazzoca/wiki/commits/master/src/machine_learning/gradient_descent.md">History</a>
          </div>
          <div id="content">
          <h1>Gradient Descent</h1>
          <div id="toc">
          </div>
          <script type="text/x-mathjax-config">
  MathJax.Hub.Config({ tex2jax: { inlineMath: [['$','$'], ["\\(","\\)"]] } });
</script>


<script type="text/javascript"
  src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML">
</script>


<h2>Batch Gradient Descent</h2>

<p>最急降下法において、パラメータの偏微分を求める際に、全ての学習データから算出する方法を、バッチ最急降下法 <em>Batch gradient descent</em> と呼ぶ。</p>

<p>この方法は、概ね最短距離で収束するが、学習データ数 <code>m</code> でパラメータ数 <code>n</code> とした場合、一回の偏微分の算出に <code>m * n</code> の計算量が必要になる。この計算量を収束するまで反復するため、学習データ数 <code>m</code> が大きくなるにつれ、無視できないコストになる。</p>

<script type="math/tex; mode=display" id="MathJax-Element-batch_grad">
J(\theta) = \frac{1}{2m} {\sum_{i=1}^{m} (h_{\theta}(x^{(i)})-y^{(i)})^2} \\
\frac{\partial}{\partial \theta_{j}} J(\theta) = \frac{1}{m} \sum_{i=1}^{m} (h_{\theta}(x^{(i)}) - y^{(i)}) x_{j}^{(i)} \\
\theta_{j} := \theta_{j} - \alpha \left( \frac{ \partial}{ \partial \theta_{j}} J(\theta) \right) \\
</script>


<h2>Stochastic Gradient Descent</h2>

<p>一つのコスト関数で、すべての学習データから誤差平均を求めるのではなく、一つの学習データごとのコスト関数に分けて誤差を求めた後に、それらの平均を取るのも結果的には同じである。</p>

<script type="math/tex; mode=display" id="MathJax-Element-stochasitc_cost">
\text{cost$(\theta, (x^{(i)}, y^{(i)}))$} = \frac{1}{2} (h_{\theta}(x^{(i)}) - y^{(i)})^{2} \\
J(\theta) = \frac{1}{m} \sum_{i=1}^{m} \text{cost$(\theta, (x^{(i)}, y^{(i)}))$} \\
</script>


<p>一つの学習データごとのコスト関数の偏微分は、同データからの誤差のみで求められるので、計算量はパラメータ数 <code>n</code> に収まる。</p>

<script type="math/tex; mode=display" id="MathJax-Element-stochasitc_grad">
\frac{\partial}{\partial \theta_{j}} \text{cost$(\theta, (x^{(i)}, y^{(i)}))$} = (h_{\theta}(x^{(i)}) - y^{(i)}) x_{j}^{(i)} \\
\theta_{j} := \theta_{j} - \alpha \left( \frac{\partial}{\partial \theta_{j}} \text{cost$(\theta, (x^{(i)}, y^{(i)}))$} \right) \\
</script>


<p>一回のパラメータ更新の際に、すべての学習データからの偏微分ではなく、一つの学習データのみから偏微分を求めるようにすれば、全ての学習データに渡って反復した場合でも、<code>m * n</code> の計算量に収まる。</p>

<p>バッチ最急降下法のように最短距離は進まず、遠回りをしながら収束するが、やがて移動範囲は狭まり収束する。この方法を、確率的最急降下法 <em>Stochastic gradient descent</em> よ呼ぶ。</p>

<div class="highlight"><pre><code class="language-octave" data-lang="octave"><span class="n">m</span> <span class="p">=</span> <span class="mi">100000</span><span class="p">;</span>
<span class="n">n</span> <span class="p">=</span> <span class="mi">4</span><span class="p">;</span>

<span class="c">% Training set X in m x n</span>
<span class="n">X</span> <span class="p">=</span> <span class="nb">repmat</span><span class="p">(</span><span class="nb">randperm</span><span class="p">(</span><span class="mi">100</span><span class="p">)</span><span class="o">&#39;</span><span class="p">,</span> <span class="n">m</span> <span class="o">/</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">1</span><span class="p">);</span>
<span class="n">X</span> <span class="p">=</span> <span class="p">[</span><span class="nb">repmat</span><span class="p">(</span><span class="nb">randperm</span><span class="p">(</span><span class="mi">100</span><span class="p">)</span><span class="o">&#39;</span><span class="p">,</span> <span class="n">m</span> <span class="o">/</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">X</span><span class="p">];</span>
<span class="n">X</span> <span class="p">=</span> <span class="p">[</span><span class="nb">repmat</span><span class="p">(</span><span class="nb">randperm</span><span class="p">(</span><span class="mi">100</span><span class="p">)</span><span class="o">&#39;</span><span class="p">,</span> <span class="n">m</span> <span class="o">/</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">X</span><span class="p">];</span>
<span class="n">X</span> <span class="p">=</span> <span class="p">[</span><span class="nb">ones</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">X</span><span class="p">];</span>
<span class="n">X</span> <span class="p">=</span> <span class="n">X</span><span class="p">(</span><span class="nb">randperm</span><span class="p">(</span><span class="n">m</span><span class="p">),</span> <span class="p">:);</span> <span class="c">% Shuffle X</span>
<span class="c">% h(x) = 1 + x1 * 2 + x2 * 3 + x3 * 4</span>
<span class="n">y</span> <span class="p">=</span> <span class="mi">1</span> <span class="o">+</span> <span class="p">(</span><span class="n">X</span><span class="p">(:,</span> <span class="mi">2</span><span class="p">)</span> <span class="o">.*</span> <span class="mi">2</span><span class="p">)</span> <span class="o">+</span> <span class="p">(</span><span class="n">X</span><span class="p">(:,</span> <span class="mi">3</span><span class="p">)</span> <span class="o">.*</span> <span class="mi">3</span><span class="p">)</span> <span class="o">+</span> <span class="p">(</span><span class="n">X</span><span class="p">(:,</span> <span class="mi">4</span><span class="p">)</span> <span class="o">.*</span> <span class="mi">4</span><span class="p">);</span>

<span class="c">% Batch gradient descent</span>
<span class="nb">fprintf</span><span class="p">(</span><span class="s">&#39;Press enter to run batch gradient descent.\n&#39;</span><span class="p">);</span>
<span class="nb">pause</span><span class="p">;</span>
<span class="n">alpha</span> <span class="p">=</span> <span class="mf">0.0001</span><span class="p">;</span>
<span class="n">theta</span> <span class="p">=</span> <span class="nb">ones</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="mi">1</span><span class="p">);</span> 
<span class="k">for</span> <span class="n">i</span> <span class="p">=</span> <span class="mi">1</span><span class="p">:</span><span class="mi">10000</span>
  <span class="n">theta</span> <span class="p">=</span> <span class="n">theta</span> <span class="o">-</span> <span class="p">(((</span><span class="n">X</span> <span class="o">*</span> <span class="n">theta</span> <span class="o">-</span> <span class="n">y</span><span class="p">)</span><span class="o">&#39;</span> <span class="o">*</span> <span class="n">X</span><span class="p">)</span> <span class="o">.*</span> <span class="n">alpha</span> <span class="o">/</span> <span class="n">m</span><span class="p">)</span><span class="o">&#39;</span><span class="p">;</span>
<span class="k">end</span>
<span class="n">theta</span>

<span class="c">% Stochastic gradient descent</span>
<span class="nb">fprintf</span><span class="p">(</span><span class="s">&#39;Press enter to run stochastic gradient descent.\n&#39;</span><span class="p">);</span>
<span class="nb">pause</span><span class="p">;</span>
<span class="n">a1</span> <span class="p">=</span> <span class="mf">0.001</span><span class="p">;</span>
<span class="n">a2</span> <span class="p">=</span> <span class="mf">0.0001</span><span class="p">;</span>
<span class="n">a3</span> <span class="p">=</span> <span class="mi">10</span><span class="p">;</span>
<span class="n">theta</span> <span class="p">=</span> <span class="nb">ones</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="mi">1</span><span class="p">);</span> 
<span class="k">for</span> <span class="n">i</span> <span class="p">=</span> <span class="mi">1</span><span class="p">:</span><span class="n">m</span> 
  <span class="n">a</span> <span class="p">=</span> <span class="n">a1</span> <span class="o">/</span> <span class="p">(</span><span class="n">i</span> <span class="o">*</span> <span class="n">a2</span> <span class="o">+</span> <span class="n">a3</span><span class="p">);</span>
  <span class="n">df</span> <span class="p">=</span> <span class="p">(</span><span class="n">X</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="p">:)</span> <span class="o">*</span> <span class="n">theta</span><span class="p">)</span> <span class="o">-</span> <span class="n">y</span><span class="p">(</span><span class="n">i</span><span class="p">);</span>
  <span class="n">theta</span> <span class="p">=</span> <span class="n">theta</span> <span class="o">-</span> <span class="p">(</span><span class="n">X</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="p">:)</span> <span class="o">.*</span> <span class="n">df</span> <span class="o">.*</span> <span class="n">a</span><span class="p">)</span><span class="o">&#39;</span><span class="p">;</span>
<span class="k">end</span>
<span class="n">theta</span></code></pre></div>


<ul>
<li>学習データは事前にシャッフルしておく。何かしらでソートされていると、片寄った動きになりうまく収束しない。</li>
<li>ある程度のデータ量であれば、学習データを一度走査するだけで、十分に良い結果が得られる。収束する余地があれば、もう一度繰り返せば良い。</li>
</ul>


<p>確率的最急降下法は、遠回りしながら収束するので、発散が起こりやすく <code>α</code> の値を小さめにする必要がある。固定値ではなく、回数を重ねる程 <code>α</code> の値が小さくなるように調整することで、効率さを高めることができる。</p>

<script type="math/tex; mode=display" id="MathJax-Element-stochasitc_grad_alpha">
{\scriptsize \text{$n = $ number of iteration}} \\
\alpha = \frac{\alpha_1}{n \cdot \alpha_2 + \alpha_3}
</script>


          </div>
          
        </div>
      </div>
    </div>
  </div>
  <!--/#main-->
  <div id="footer">
<div class="container-fluid">
  <div class="row">
    <div class="col-md-12">
      <dl class="dl-horizontal pull-right">
        <dt><span class="muted">Repository</span></dt>
        <dd><a href="https://github.com/tachesimazzoca/wiki">tachesimazzoca/wiki</a></dd>
        <dt><span class="muted">Author</span></dt>
        <dd><a href="https://github.com/tachesimazzoca">@tachesimazzoca</a></dd>
      </dl>
    </div>
  </div>
</div>
</div>
<!--/#footer-->

</div>
<!--/#wrapper-->
<!--Google Analytics-->
<script type="text/javascript">

if ("tachesimazzoca.github.io" == location.hostname) {

  var _gaq = _gaq || [];
  _gaq.push(['_setAccount', 'UA-35398490-1']);
  _gaq.push(['_trackPageview']);

  (function() {
    var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
    ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
    var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
  })();
}

</script>
<!--/Google Analytics-->

</body>
</html>

