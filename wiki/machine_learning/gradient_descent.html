<!DOCTYPE html>
<html>
<head>
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1">
<title>tachesimazzoca - Wiki | Gradient Descent</title>
<link href="../assets/lib/bootstrap-3.2.0/css/bootstrap.min.css" media="screen" rel="stylesheet" type="text/css">
<link href="../assets/stylesheets/style.css" media="screen" rel="stylesheet" type="text/css">
<script src="../assets/lib/jquery-1.11.1/jquery.min.js"></script>
<script src="../assets/lib/bootstrap-3.2.0/js/bootstrap.min.js"></script>
<!-- Google Tag Manager -->
<script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
})(window,document,'script','dataLayer','GTM-M96M8MG');</script>
<!-- End Google Tag Manager -->

</head>
<body>
<div id="wrapper">
<div id="header">
<div class="navbar navbar-static-top navbar-inverse">
<div class="container-fluid">
<div class="navbar-header">
<button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#jsNavbarCollapse-1">
<span class="sr-only">Toggle navigation</span>
<span class="icon-bar"></span>
<span class="icon-bar"></span>
<span class="icon-bar"></span>
</button>
<a class="navbar-brand" href="../index.html">tachesimazzoca - Wiki</a>
</div>
<div class="collapse navbar-collapse" id="jsNavbarCollapse-1">
<ul class="nav navbar-nav">
</ul>
<form class="navbar-form navbar-right" action="https://www.google.com/search" style="box-shadow: none; border: none">
<input type="hidden" name="as_sitesearch" value="tachesimazzoca.github.io/wiki">
<div class="form-group">
<div class="input-group">
<div class="input-group-addon" style="background: transparent; border-color: #333">
<span class="glyphicon glyphicon-search"></span>
</div>
<input type="text" class="form-control" name="as_q" placeholder="Search">
</div>
</div>
</form>

</div>
</div>
</div>
</div>
<!--/#header-->

<div id="main">
<div class="container-fluid">
<div class="pankuzu">
<ul>
  <li><a href="../index.html">Home</a></li>
  <li><a href="../machine_learning/index.html">Machine Learning</a></li>
  <li>Gradient Descent</li>
</ul>
</div>
</div>
<div class="container-fluid">
<div class="row">
<div class="col-md-3 col-sm-4 hidden-xs">
<div id="navigation" data-spy="affix" data-offset-top="110">
<div class="panel panel-default">
<div class="panel-heading">
<h3 class="panel-title" style="text-align: center; font-size: 11px; text-transform: uppercase; color: #999">Table of Contents</h3>
</div>
<div class="panel-body" id="jsNavigationHolder">
<ul>
  <li><a href="../machine_learning/gradient_descent.html#gradient-descent" class="header">Gradient Descent</a>
  <ul>
    <li><a href="../machine_learning/gradient_descent.html#batch-gradient-descent" class="header">Batch Gradient Descent</a></li>
    <li><a href="../machine_learning/gradient_descent.html#stochastic-gradient-descent" class="header">Stochastic Gradient Descent</a></li>
    <li><a href="../machine_learning/gradient_descent.html#mini-batch-gradient-descent" class="header">Mini-batch Gradient Descent</a></li>
    <li><a href="../machine_learning/gradient_descent.html#data-parallelism" class="header">Data Parallelism</a></li>
  </ul></li>
</ul>
</div>
</div>

</div>
</div>
<div class="col-md-9 col-sm-8">
<div class="btn-group pull-right">
<a class="btn btn-info" href="https://github.com/tachesimazzoca/wiki/tree/master/src/main/paradox/machine_learning/gradient_descent.md">Source</a>
</div>
<div id="content">
<h1><a href="#gradient-descent" name="gradient-descent" class="anchor"><span class="anchor-link"></span></a>Gradient Descent</h1>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({ tex2jax: { inlineMath: [['$','$'], ["\\(","\\)"]] } });
</script>
<script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS_HTML">
</script>
<h2><a href="#batch-gradient-descent" name="batch-gradient-descent" class="anchor"><span class="anchor-link"></span></a>Batch Gradient Descent</h2>
<p>最急降下法において、パラメータの偏微分を求める際に、全ての学習データから算出する方法を、バッチ最急降下法 <em>Batch gradient descent</em> と呼ぶ。</p>
<p>この方法は、概ね最短距離で収束するが、学習データ数 <code>m</code> でパラメータ数 <code>n</code> とした場合、一回の偏微分の算出に <code>m * n</code> の計算量が必要になる。この計算量を収束するまで反復するため、学習データ数 <code>m</code> が大きくなるにつれ、無視できないコストになる。</p>
<script type="math/tex; mode=display" id="MathJax-Element-batch_grad">
J(\theta) = \frac{1}{2m} {\sum_{i=1}^{m} (h_{\theta}(x^{(i)})-y^{(i)})^2} \\
\frac{\partial}{\partial \theta_{j}} J(\theta) = \frac{1}{m} \sum_{i=1}^{m} (h_{\theta}(x^{(i)}) - y^{(i)}) x_{j}^{(i)} \\
\theta_{j} := \theta_{j} - \alpha \left( \frac{ \partial}{ \partial \theta_{j}} J(\theta) \right) \\
</script>
<h2><a href="#stochastic-gradient-descent" name="stochastic-gradient-descent" class="anchor"><span class="anchor-link"></span></a>Stochastic Gradient Descent</h2>
<p>一つのコスト関数で、すべての学習データから誤差平均を求めるのではなく、一つの学習データごとのコスト関数に分けて誤差を求めた後に、それらの平均を取るのも結果的には同じである。</p>
<script type="math/tex; mode=display" id="MathJax-Element-stochasitc_cost">
\text{cost$(\theta, (x^{(i)}, y^{(i)}))$} = \frac{1}{2} (h_{\theta}(x^{(i)}) - y^{(i)})^{2} \\
J(\theta) = \frac{1}{m} \sum_{i=1}^{m} \text{cost$(\theta, (x^{(i)}, y^{(i)}))$} \\
</script>
<p>一つの学習データごとのコスト関数の偏微分は、同データからの誤差のみで求められるので、計算量はパラメータ数 <code>n</code> に収まる。</p>
<script type="math/tex; mode=display" id="MathJax-Element-stochasitc_grad">
\frac{\partial}{\partial \theta_{j}} \text{cost$(\theta, (x^{(i)}, y^{(i)}))$} = (h_{\theta}(x^{(i)}) - y^{(i)}) x_{j}^{(i)} \\
\theta_{j} := \theta_{j} - \alpha \left( \frac{\partial}{\partial \theta_{j}} \text{cost$(\theta, (x^{(i)}, y^{(i)}))$} \right) \\
</script>
<p>一回のパラメータ更新の際に、すべての学習データからの偏微分ではなく、一つの学習データのみから偏微分を求めるようにすれば、全ての学習データに渡って反復した場合でも、<code>m * n</code> の計算量に収まる。</p>
<p>バッチ最急降下法のように最短距離は進まず、遠回りをしながら収束するが、やがて移動範囲は狭まり収束する。この方法を、確率的最急降下法 <em>Stochastic gradient descent</em> よ呼ぶ。</p>
<pre class="prettyprint"><code class="language-octave">m = 100000;
n = 4;

% Training set X in m x n
X = repmat(randperm(100)&#39;, m / 100, 1);
X = [repmat(randperm(100)&#39;, m / 100, 1), X];
X = [repmat(randperm(100)&#39;, m / 100, 1), X];
X = [ones(m, 1), X];
X = X(randperm(m), :); % Shuffle X
% h(x) = 1 + x1 * 2 + x2 * 3 + x3 * 4
y = 1 + (X(:, 2) .* 2) + (X(:, 3) .* 3) + (X(:, 4) .* 4);

% Batch gradient descent
fprintf(&#39;Press enter to run batch gradient descent.\n&#39;);
pause;
alpha = 0.0001;
theta = ones(n, 1);
for i = 1:10000
  theta = theta - (((X * theta - y)&#39; * X) .* alpha / m)&#39;;
end
theta

% Stochastic gradient descent
fprintf(&#39;Press enter to run stochastic gradient descent.\n&#39;);
pause;
a1 = 0.001;
a2 = 0.0001;
a3 = 10;
theta = ones(n, 1);
for i = 1:m
  a = a1 / (i * a2 + a3);
  df = (X(i, :) * theta) - y(i);
  theta = theta - (X(i, :) .* df .* a)&#39;;
end
theta
</code></pre>
<ul>
  <li>学習データは事前にシャッフルしておく。何かしらでソートされていると、片寄った動きになりうまく収束しない。</li>
  <li>ある程度のデータ量であれば、学習データを一度走査するだけで、十分に良い結果が得られる。収束する余地があれば、もう一度繰り返せば良い。</li>
</ul>
<p>確率的最急降下法は、遠回りしながら収束するので、発散が起こりやすく <code>α</code> の値を小さめにする必要がある。固定値ではなく、回数を重ねる程 <code>α</code> の値が小さくなるように調整することで、効率さを高めることができる。</p>
<script type="math/tex; mode=display" id="MathJax-Element-stochasitc_grad_alpha">
{\scriptsize \text{$n = $ number of iteration}} \\
\alpha = \frac{\alpha_1}{n \cdot \alpha_2 + \alpha_3}
</script>
<p>この方法を用いれば、永続的なオンライン学習が可能になる。サンプルを得られた時に、パラメータを更新すればよく、データを保存する必要もない。常にストリームで流れているケースで有効である。</p>
<ul>
  <li>Web サイトでの検索結果に対し「クリックする / しない」を予測して、よりクリックされやすい結果を上位に表示する。</li>
  <li>ECサイトにおいての販売価格に対し「買う / 買わない」を予測して、適切な価格帯を調べる。</li>
</ul>
<h2><a href="#mini-batch-gradient-descent" name="mini-batch-gradient-descent" class="anchor"><span class="anchor-link"></span></a>Mini-batch Gradient Descent</h2>
<p>確率的最急降下法では、一つの学習データから偏微分を求めていたが、10 サンプル程度の単位でまとめる方法もある。</p>
<script type="math/tex; mode=display" id="MathJax-Element-mini_batch_grad">
\text{for $i = 1, 11, 21, 31, \ldots$} \\
\theta_{j} := \theta_{j} - \alpha \frac{1}{10} \sum_{k=i}^{i+9} (h_{\theta}(x^{(k)}) - y^{(k)}) x_{j}^{(k)} \\
</script>
<h2><a href="#data-parallelism" name="data-parallelism" class="anchor"><span class="anchor-link"></span></a>Data Parallelism</h2>
<p>バッチ最急降下法での偏微分は、総和から平均を取っているので、総和の算出部分を分散できる。</p>
<script type="math/tex; mode=display" id="MathJax-Element-data_parallelism_grad">
\frac{\partial}{\partial \theta_{j}} J(\theta) = \frac{1}{m} \sum_{i=1}^{m} (h_{\theta}(x^{(i)}) - y^{(i)}) x_{j}^{(i)} = \frac{1}{m} s_{j} \\
s_{j} = \sum_{i=1}^{m} (h_{\theta}(x^{(i)}) - y^{(i)}) x_{j}^{(i)} \\
</script>
<p><em>MapReduce</em> であれば <em>Mapper</em> に学習データを振り分けて総和のみを算出したのち <em>Reducer</em> で各 <em>Mapper</em> からの総和を合算して平均を求めることができる。</p>
<script type="math/tex; mode=display" id="MathJax-Element-data_parallelism_mapred">
{\scriptsize \text{$m = 80,000$}} \\
\begin{align}
s_{j}^{(1)} & = \sum_{i=1}^{10,000} (h_{\theta}(x^{(i)}) - y^{(i)}) x_{j}^{(i)} & \text{mapper1} \\
s_{j}^{(2)} & = \sum_{i=10,001}^{20,000} (h_{\theta}(x^{(i)}) - y^{(i)}) x_{j}^{(i)} & \text{mapper2} \\
& \vdots & \\
s_{j}^{(8)} & = \sum_{i=70,001}^{80,000} (h_{\theta}(x^{(i)}) - y^{(i)}) x_{j}^{(i)} & \text{mapper8} \\
\theta_{j} & := \theta_{j} - \alpha \frac{1}{m} \sum_{k=1}^{8} s_{j}^{(k)} & \text{reducer} \\
\end{align} \\
</script>
</div>
</div>
</div>
</div>
</div>
<!--/#main-->
<div id="footer">
<div class="container-fluid">
<div class="row">
<div class="col-md-12">
<dl class="dl-horizontal pull-right">
<dt><span class="muted">Repository</span></dt>
<dd><a href="https://github.com/tachesimazzoca/wiki">tachesimazzoca/wiki</a></dd>
<dt><span class="muted">Author</span></dt>
<dd><a href="https://github.com/tachesimazzoca">@tachesimazzoca</a></dd>
</dl>
</div>
</div>
</div>
</div>
<!--/#footer-->

</div>
<!--/#wrapper-->
<style type="text/css">@import "../assets/lib/prettify/prettify.css";</style>
<script type="text/javascript" src="../assets/lib/prettify/prettify.js"></script>
<script type="text/javascript">
(function(jq) {
jq(function(){
window.prettyPrint && prettyPrint();
});
})(window.jQuery);
</script>
</body>
</html>
