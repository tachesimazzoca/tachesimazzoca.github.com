<!DOCTYPE html>
<html>
<head>
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1">
<title>tachesimazzoca - Wiki | Neural Networks</title>
<link href="../assets/lib/bootstrap-3.2.0/css/bootstrap.min.css" media="screen" rel="stylesheet" type="text/css">
<link href="../assets/stylesheets/style.css" media="screen" rel="stylesheet" type="text/css">
<script src="../assets/lib/jquery-1.11.1/jquery.min.js"></script>
<script src="../assets/lib/bootstrap-3.2.0/js/bootstrap.min.js"></script>
<!-- Google Tag Manager -->
<script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
})(window,document,'script','dataLayer','GTM-M96M8MG');</script>
<!-- End Google Tag Manager -->

</head>
<body>
<div id="wrapper">
<div id="header">
<div class="navbar navbar-static-top navbar-inverse">
<div class="container-fluid">
<div class="navbar-header">
<button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#jsNavbarCollapse-1">
<span class="sr-only">Toggle navigation</span>
<span class="icon-bar"></span>
<span class="icon-bar"></span>
<span class="icon-bar"></span>
</button>
<a class="navbar-brand" href="../index.html">tachesimazzoca - Wiki</a>
</div>
<div class="collapse navbar-collapse" id="jsNavbarCollapse-1">
<ul class="nav navbar-nav">
</ul>
<form class="navbar-form navbar-right" action="http://www.google.com/search" style="box-shadow: none; border: none">
<input type="hidden" name="as_sitesearch" value="tachesimazzoca.github.io/wiki">
<div class="form-group">
<div class="input-group">
<div class="input-group-addon" style="background: transparent; border-color: #333">
<span class="glyphicon glyphicon-search"></span>
</div>
<input type="text" class="form-control" name="as_q" placeholder="Search">
</div>
</div>
</form>

</div>
</div>
</div>
</div>
<!--/#header-->

<div id="main">
<div class="container-fluid">
<div class="pankuzu">
<ul>
  <li><a href="../index.html">Home</a></li>
  <li><a href="../machine_learning/index.html">Machine Learning</a></li>
  <li>Neural Networks</li>
</ul>
</div>
</div>
<div class="container-fluid">
<div class="row">
<div class="col-md-3 col-sm-4 hidden-xs">
<div id="navigation" data-spy="affix" data-offset-top="110">
<div class="panel panel-default">
<div class="panel-heading">
<h3 class="panel-title" style="text-align: center; font-size: 11px; text-transform: uppercase; color: #999">Table of Contents</h3>
</div>
<div class="panel-body" id="jsNavigationHolder">
<ul>
  <li><a href="../machine_learning/neural_networks.html#neural-networks" class="header">Neural Networks</a>
  <ul>
    <li><a href="../machine_learning/neural_networks.html#sigmoid-activation-function" class="header">Sigmoid Activation Function</a></li>
    <li><a href="../machine_learning/neural_networks.html#forward-propagation" class="header">Forward Propagation</a></li>
    <li><a href="../machine_learning/neural_networks.html#multi-class-classification" class="header">Multi-class Classification</a></li>
    <li><a href="../machine_learning/neural_networks.html#cost-function" class="header">Cost Function</a></li>
    <li><a href="../machine_learning/neural_networks.html#sigmoid-gradient-function" class="header">Sigmoid Gradient Function</a></li>
    <li><a href="../machine_learning/neural_networks.html#backpropagation" class="header">Backpropagation</a></li>
  </ul></li>
</ul>
</div>
</div>

</div>
</div>
<div class="col-md-9 col-sm-8">
<div id="content">
<h1><a href="#neural-networks" name="neural-networks" class="anchor"><span class="anchor-link"></span></a>Neural Networks</h1>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({ tex2jax: { inlineMath: [['$','$'], ["\\(","\\)"]] } });
</script>
<script type="text/javascript"
  src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML">
</script>
<h2><a href="#sigmoid-activation-function" name="sigmoid-activation-function" class="anchor"><span class="anchor-link"></span></a>Sigmoid Activation Function</h2>
<p>入力とパラメータの内積を、シグモイド関数を通して <code>0 &lt; a &lt; 1</code> の範囲に変換するモデルを、 <em>Sigmoid (Logistic) activation function</em> と呼ぶ。</p>
<script type="math/tex; mode=display" id="MathJax-Element-logistic_unit">
g(z) = \frac{1}{1 + e^{-z} } \\
h_{\theta}(x) = g({\theta}^{T} x) \\
</script>
<p>バイアス項 <em>Bias unit</em> として、<code>x(0) = 1</code> で固定し、<code>theta(0)</code> をオフセット値とする。入力値 <code>x</code> を <code>(0|1)</code> に制限すると、<code>theta</code> の値に応じて AND / OR の論理回路を実現できる。</p>
<pre class="prettyprint"><code class="language-octave">X = [0 0; 0 1; 1 0; 1 1];
X = [ones(length(X), 1), X];         % [1 0 0; 1 0 1; 1 1 0; 1 1 1]

theta1 = [-30; 20; 20];              % AND gate
z1 = X * theta1;                     % [-30; -10; -10; 10]
h1 = sigmoid(z1);                    % [0; 0; 0; 1]

theta2 = [-10; 20; 20];              % OR gate
z2 = X * theta2;                     % [-10; 10; 10; 30]
h2 = sigmoid(z2);                    % [0; 1; 1; 1]
</code></pre>
<h2><a href="#forward-propagation" name="forward-propagation" class="anchor"><span class="anchor-link"></span></a>Forward Propagation</h2>
<p><em>Activation function</em> を複数レイヤーに定義し、各レイヤーの出力を次のレイヤーへの入力とすることで、より複雑な論理回路を実現できる。このモデルは、神経回路のシミュレーションを元にしており、ニューラルネットワーク <em>Neural network</em> と呼ばれる。</p>
<pre><code>   L1    |    L2    |    L3    |    L4
------------------------------------------
    [+1]-+     [+1]-+     [+1]-+
         |          |          |
  [x(1)]-+-&gt;[a2(1)]-+-&gt;[a3(1)]-+-&gt;[a4(1)]-&gt; h(x)
         |          |          |
  [x(2)]-+-&gt;[a2(2)]-+-&gt;[a3(2)]-+
         |          |
         +-&gt;[a2(3)]-+
</code></pre>
<script type="math/tex; mode=display" id="MathJax-Element-neural_network_layer2">
\begin{align}
\text{Input} \quad & \left\{
  \begin{array}{l l}
    x_0 = 1 \\
    x_1 \in \mathbb{R} \\
    x_2 \in \mathbb{R} \\
  \end{array}
\right. \\

\text{Layer1} \quad & \left\{
  \begin{array}{l l}
    {\Theta}^{(1)} \in \mathbb{R}^{3 \times 3} \\
    a^{(2)}_0 = 1 \\
    a^{(2)}_{1} = g({ {\Theta}^{(1)}_{1,0} } x_0 + { {\Theta}^{(1)}_{1,1} } x_1 + { {\Theta}^{(1)}_{1,2} x_2 }) \\
    a^{(2)}_{2} = g({ {\Theta}^{(1)}_{2,0} } x_0 + { {\Theta}^{(1)}_{2,1} } x_1 + { {\Theta}^{(1)}_{2,2} x_2 }) \\
    a^{(2)}_{3} = g({ {\Theta}^{(1)}_{3,0} } x_0 + { {\Theta}^{(1)}_{3,1} } x_1 + { {\Theta}^{(1)}_{3,2} x_2 }) \\
  \end{array}
\right. \\

\text{Layer2} \quad & \left\{
  \begin{array}{l l}
    {\Theta}^{(2)} \in \mathbb{R}^{2 \times 4} \\
    a^{(3)}_0 = 1 \\
    a^{(3)}_{1} = g({\Theta}^{(2)}_{1,0} a^{(2)}_0 + {\Theta}^{(2)}_{1,1} a^{(2)}_1 + {\Theta}^{(2)}_{1,2} a^{(2)}_2 + {\Theta}^{(2)}_{1,3} a^{(2)}_3) \\
    a^{(3)}_{2} = g({\Theta}^{(2)}_{2,0} a^{(2)}_0 + {\Theta}^{(2)}_{2,1} a^{(2)}_1 + {\Theta}^{(2)}_{2,2} a^{(2)}_2 + {\Theta}^{(2)}_{2,3} a^{(2)}_3) \\
  \end{array}
\right. \\

\text{Layer3} \quad & \left\{
  \begin{array}{l l}
    {\Theta}^{(3)} \in \mathbb{R}^{1 \times 3} \\
    a^{(4)}_1 = g({\Theta}^{(3)}_{1,0} a^{(3)}_0 + {\Theta}^{(3)}_{1,1} a^{(3)}_1 + {\Theta}^{(3)}_{1,2} a^{(3)}_2) \\
    h_{\Theta}(x) = a^{(4)}_1 \\
  \end{array}
\right. \\

\end{align}
</script>
<ul>
  <li>入力 <code>x</code> のバイアス項として <code>x(0) = 1</code> とする。</li>
  <li>Layer1: ３つの入力 <code>x</code> から、３つの出力 <code>a2</code> を算出する。バイアス項として <code>a2(0) = 1</code> とする。</li>
  <li>Layer2: Layer1 の４つの出力 <code>a2</code> を入力とし、２つの出力 <code>a3</code> を算出する。バイアス項として <code>a3(0) = 1</code> とする。</li>
  <li>Layer3: Layer2 の３つの出力 <code>a3</code> を入力とし、１つの出力 <code>a4</code> を算出する。</li>
  <li>最終レイヤーの出力 <code>a4(1)</code> が、予測値 <code>h(x)</code> となる。</li>
</ul>
<p>Layer2 の２入力に NAND / OR ゲート、Layer3 の１入力に AND ゲートを置くことで、XOR ゲートとして機能する。</p>
<pre class="prettyprint"><code class="language-octave">Theta1 = [30 -20 -20; -10 20 20];    % [{NAND}; {OR}]
Theta2 = [-30 20 20];                % [{AND}]

X = [0 0; 0 1; 1 0; 1 1];
X = [ones(length(X), 1), X];         % [1 0 0; 1 0 1; 1 1 0; 1 1 1]

z1 = X * Theta1&#39;;                    % [30 -10; 10 10; 10 10; -10 30]
a2 = sigmoid(z1);                    % [1 0; 1 1; 1 1; 0 1]
a2 = [ones(length(a2), 1), a2];      % [1 1 0; 1 1 1; 1 1 1; 1 0 1]

z2 = a2 * Theta2&#39;;                   % [-10; 10; 10; -10]
a3 = sigmoid(z2);                    % [0; 1; 1; 0]
</code></pre>
<p>このように、複数レイヤーの入出力を介して、前方に伝播させていく方法を <em>Forward propagation</em> と呼ぶ。</p>
<h2><a href="#multi-class-classification" name="multi-class-classification" class="anchor"><span class="anchor-link"></span></a>Multi-class Classification</h2>
<p>３つ以上に分類するには、出力レイヤーのユニットを、分類の数だけ用意すればよい。</p>
<pre><code>   L1    |    L2    |    L3    |
--------------------------------
    [+1]-+     [+1]-+
         |          |
  [x(1)]-+-&gt;[a2(1)]-+-&gt;[a3(1)]
         |          |
  [x(2)]-+-&gt;[a2(2)]-+-&gt;[a3(2)]
         |          |
         +-&gt;[a2(3)]-+-&gt;[a3(3)]
         |          |
         +-&gt;[a2(4)]-+
</code></pre>
<p>出力レイヤーのベクトルを <code>a3</code>、正解値を <code>y</code> としたとき、<code>a3(y)</code> のフラグが立つと考える。</p>
<ul>
  <li><code>y = 1 if a3 = [1; 0; 0]</code></li>
  <li><code>y = 2 if a3 = [0; 1; 0]</code></li>
  <li><code>y = 3 if a3 = [0; 0; 1]</code></li>
</ul>
<p>分類数を <code>K</code> とすると、コスト関数は各出力の誤差平均を求めればよい。</p>
<script type="math/tex; mode=display" id="MathJax-Element-backprop_cost">
a = h_{\Theta}(x) \in \mathbb{R}^{K}\\
J(\Theta) = \frac{1}{m} {\sum_{i=1}^{m}} {\sum_{k=1}^{K}} [ -log(a_{k}^{(i)})(y_{k}^{(i)}) - log(1 - a_{k}^{(i)}) (1 - y_{k}^{(i)}) ] \\
</script>
<h2><a href="#cost-function" name="cost-function" class="anchor"><span class="anchor-link"></span></a>Cost Function</h2>
<p>ニューラルネットワークのコスト関数は、ロジスティック回帰と同様であるが、予測値を求めるには <em>Forward propagation</em> で各レイヤーを通して算出する必要がある。</p>
<pre class="prettyprint"><code class="language-octave">Theta1 = [-30 10 10 10; -10 20 20 20; 20 -10 -10 -10; -20 10 10 10];
Theta2 = [10 -20 -20 -10 -10; -10 10 10 0 10; 20 -20 -20 -10 -10];
X = [0 0 0; 0 0 1; 0 1 0; 1 1 1];
m = size(X, 1);

a1 = X;                         % 4 x 3
a1 = [ones(m, 1) a1];           % 4 x 4
z2 = a1 * Theta1&#39;;              % 4 x 4
a2 = sigmoid(z2);
a2 = [ones(size(a2, 1), 1) a2]; % 4 x 5
z3 = a2 * Theta2&#39;;              % 4 x 3
h = sigmoid(z3);
</code></pre>
<p><em>Regularization</em> を行なう場合は、各レイヤーにパラメータがある点に注意する。バイアス項は除外する。</p>
<script type="math/tex; mode=display" id="MathJax-Element-backprop_cost_reg">
J(\Theta) = J(\Theta) + \frac{\lambda}{2m} \sum_{l=1}^{L-1} \sum_{i=1}^{sl} \sum_{j=1}^{sl+1} ({\Theta}_{j,i}^{(l)})^2 \\
{\scriptsize \text{$L = $ the number of layers}} \\
{\scriptsize \text{$sl = $ the number of parameters of the layer $l$}} \\
</script>
<pre class="prettyprint"><code class="language-octave">t1 = Theta1;
t2 = Theta2;
t1(:, 1) = 0;
t2(:, 1) = 0;

lambda = 0.1;
J = J + (sum(sum(t1 .^ 2)) + sum(sum(t2 .^ 2))) * lambda / (2 * m);
</code></pre>
<h2><a href="#sigmoid-gradient-function" name="sigmoid-gradient-function" class="anchor"><span class="anchor-link"></span></a>Sigmoid Gradient Function</h2>
<p>ネイピア数 <code>e</code> を底とする指数の微分は <code>(e^x)&#39; = e^x</code> であることを利用して、シグモイド関数 <code>g(z)</code> を微分すると <code>g&#39;(z) = g(z) * (1 - g(z))</code> となる。</p>
<script type="math/tex; mode=display" id="MathJax-Element-sigmoid_partial_simplify">
g(z) = \frac{1}{1 + e^{-z}} \\

\begin{align}

& \left\{
\begin{array}{l l}
x = -z \\
u = 1 + e^{x} \\
g'(u) = (u^{-1})' = -1 \cdot u^{-2} = -(1 + e^{-z})^{-2} \\
u' = (1 + e^{x})' = (e^{x})' = (e^{x})'(x)' = (e^{-z})'(-z)' = (e^{-z})(-1) = -e^{-z} \\
\end{array}
\right. \\

\end{align} \\

</script>
<script type="math/tex; mode=display" id="MathJax-Element-sigmoid_gradient">
\begin{align}
g'(z) & = g'(u) \cdot u' = -(1 + e^{-z})^{-2} \cdot -e^{-z}\\
      & = \frac{e^{-z}}{(1 + e^{-z})^2} \\
      & = \frac{1}{1 + e^{-z}} \left( \frac{1 + e^{-z}}{1 + e^{-z}} - \frac{1}{1 + e^{-z}} \right) \\
      & = \frac{1}{1 + e^{-z}} \left( 1 - \frac{1}{1 + e^{-z}} \right) \\
      & = g(z)(1 - g(z)) \\
g'(0) & = g(0)(1 - g(0)) = 0.5 \cdot 0.5 = 0.25 \\
\end{align} \\
</script>
<h2><a href="#backpropagation" name="backpropagation" class="anchor"><span class="anchor-link"></span></a>Backpropagation</h2>
<p>ニューラルネットワークの各ユニットのパラメータ（ニューロンの重み）を求めるには、ロジスティック回帰と同様に最急降下法を用いる。各ユニットの偏微分を求めるためには、最終出力の誤差から各レイヤーを逆に伝播して算出する必要がある。この方法を、誤差逆伝播法 <em>Backpropagation</em> と呼ぶ。</p>
<p>出力レイヤーの誤差は、予測値ベクトルから正解値ベクトルを引いたものになる。</p>
<script type="math/tex; mode=display" id="MathJax-Element-backprop_error_output">
\delta^{(L)}_{k} = a^{(L)}_{k} - y_{k}\\
</script>
<p>中間レイヤーの誤差は以下の式で求められる。各パラメータ自身が次のレイヤーに伝播させてしまった誤差を算出すると考えればよい。バイアス項は含めなくてよい。</p>
<script type="math/tex; mode=display" id="MathJax-Element-backprop_error_hidden">
\delta^{(l)} = ({\Theta}^{(l)})^{T} \delta^{(l+1)} .* g'(z^{(l)}) \quad {\scriptsize \text{(Remove $\delta^{(l)}_0$)}} \\

\left\{
  \begin{array}{l l}
    \delta^{(l)}_1 = ({\Theta}^{(l)}_{1,1} \delta^{(l+1)}_{1} + {\Theta}^{(l)}_{2,1} \delta^{(l+1)}_{2} + {\Theta}^{(l)}_{3,1} \delta^{(l+1)}_{3} \ldots) \cdot g'(z^{(l)}_1) \\
    \delta^{(l)}_2 = ({\Theta}^{(l)}_{1,2} \delta^{(l+1)}_{1} + {\Theta}^{(l)}_{2,2} \delta^{(l+1)}_{2} + {\Theta}^{(l)}_{3,2} \delta^{(l+1)}_{3} \ldots) \cdot g'(z^{(l)}_2) \\
    \delta^{(l)}_3 = ({\Theta}^{(l)}_{1,3} \delta^{(l+1)}_{1} + {\Theta}^{(l)}_{2,3} \delta^{(l+1)}_{2} + {\Theta}^{(l)}_{3,3} \delta^{(l+1)}_{3} \ldots) \cdot g'(z^{(l)}_3) \\
  \end{array} \\
\right. \\
</script>
<p>入力レイヤーの誤差は存在しないので算出する必要はない。</p>
<p>ユニットの入力 <code>a(l)</code> に、直後のレイヤーの誤差を掛け合わせたものが、各パラメータの偏微分となる。</p>
<script type="math/tex; mode=display" id="MathJax-Element-backprop_grad">
\Delta^{(l)} = \Delta^{(l)} + \delta^{(l+1)}(a^{(l)})^{T} \\
\frac{\partial}{\partial \Theta^{(l)}_{i,j}} J(\Theta) = D^{(l)}_{i,j} = a^{(l)}_{j} \delta^{(l+1)}_{i} = \frac{1}{m} \Delta^{(l)}_{i,j} \\
</script>
<p><em>Regularization</em> を行なう場合は、各パラメータ毎にペナルティを与えればよい。コスト関数と同様に、バイアス項は除外する。</p>
<script type="math/tex; mode=display" id="MathJax-Element-backprop_grad_reg">
D^{(l)}_{i,j} = D^{(l)}_{i,j} + \frac{\lambda}{m} \Theta^{(l)}_{i,j} \\
</script>
<h3><a href="#numerical-gradient" name="numerical-gradient" class="anchor"><span class="anchor-link"></span></a>Numerical Gradient</h3>
<p><em>Backpropagation</em> を正しく行なえているかどうかは、一つのパラメータのみ極小値で増減させて、二つのコスト関数を適用した差分が <em>Backpropagation</em> で得た偏微分とほぼ相違ないこと（1e-9 以下が目安）をチェックすればよい。</p>
<script type="math/tex; mode=display" id="MathJax-Element-grad_checking">
\frac{\partial}{\partial \theta_1} J(\theta) \approx \frac{ J(\theta_1 + \epsilon, \theta_2, \theta_3, \ldots, \theta_n) - J(\theta_1 - \epsilon, \theta_2 , \theta_3, \ldots, \theta_n) }{2 \epsilon} \\
\frac{\partial}{\partial \theta_2} J(\theta) \approx \frac{ J(\theta_1, \theta_2 + \epsilon, \theta_3, \ldots, \theta_n) - J(\theta_1, \theta_2 - \epsilon, \theta_3, \ldots, \theta_n) }{2 \epsilon} \\
\ldots \\
\frac{\partial}{\partial \theta_n} J(\theta) \approx \frac{ J(\theta_1, \theta_2, \theta_3, \ldots, \theta_n + \epsilon) - J(\theta_1, \theta_2, \theta_3, \ldots, \theta_n - \epsilon) }{2 \epsilon} \\
</script>
<pre class="prettyprint"><code class="language-octave">function grad = numericalGradient(J, theta)
  m = length(theta);
  grad = zeros(m, 1);

  E = 0.01; % epsilon;
  for i = 1:m
    theta1 = theta; theta1(i) = theta1(i) + E;
    theta2 = theta; theta2(i) = theta2(i) - E;
    grad(i) = (J(theta1) - J(theta2)) / (2 * E);
  end
end
</code></pre>
<p>すべてのパラメータに対してコスト関数を適用するため、非常に処理時間がかかる。あくまで <em>Backpropagation</em> が正しくおこなえているかのチェックのみで、実際の学習処理に含めてはならない。</p>
<h3><a href="#symmetry-breaking" name="symmetry-breaking" class="anchor"><span class="anchor-link"></span></a>Symmetry Breaking</h3>
<p>ニューラルネットワークにおいては、レイヤーの出力として、次のレイヤーの各ユニットへ同じ入力が与えられる。このため入力に対するパラメータ（重み）が同じ値の場合、同一レイヤー内の全てのユニットの出力が同じ値になってしまう。</p>
<ul>
  <li>初期パラメータを全て 0 にすると、バイアス項のみが伝播する。</li>
  <li>初期パラメータを全て 1 にすると、全てのユニットの出力が、前ユニット出力の総和のシグモイド値になる。</li>
</ul>
<p>このため <em>Backpropagation</em> を開始する際の初期パラメータは、ランダムである必要がある。<code>-ε .. ε</code> の範囲でランダムに設定するとよい。</p>
<pre class="prettyprint"><code class="language-octave">E = 0.01 % epsilon
Theta1 = rand(3, 4) * (2 * E) - E; % initialize to 3 x 4 random matrix
Theta2 = rand(3, 5) * (2 * E) - E; % initialize to 3 x 5 random matrix
</code></pre>
</div>
</div>
</div>
</div>
</div>
<!--/#main-->
<div id="footer">
<div class="container-fluid">
<div class="row">
<div class="col-md-12">
<dl class="dl-horizontal pull-right">
<dt><span class="muted">Repository</span></dt>
<dd><a href="https://github.com/tachesimazzoca/wiki">tachesimazzoca/wiki</a></dd>
<dt><span class="muted">Author</span></dt>
<dd><a href="https://github.com/tachesimazzoca">@tachesimazzoca</a></dd>
</dl>
</div>
</div>
</div>
</div>
<!--/#footer-->

</div>
<!--/#wrapper-->
<style type="text/css">@import "../assets/lib/prettify/prettify.css";</style>
<script type="text/javascript" src="../assets/lib/prettify/prettify.js"></script>
<script type="text/javascript">
(function(jq) {
jq(function(){
window.prettyPrint && prettyPrint();
});
})(window.jQuery);
</script>
</body>
</html>
